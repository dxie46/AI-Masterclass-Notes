{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfbbf2c",
   "metadata": {},
   "source": [
    "## Section 1\n",
    "Hybrid models - combines different AI models together\n",
    "* CNN, MDN, GA, VAE, RNN, ES\n",
    "* Very powerful but computationally expensive to train\n",
    "\n",
    "**Useful resources:**\n",
    "<br>\n",
    "worldmodels.github.io\n",
    "<br>\n",
    "blog.otoro.net\n",
    "\n",
    "## Section 2\n",
    "\n",
    "**Neurons**\n",
    "<br>\n",
    "Recreation of a human neuron\n",
    "<br>\n",
    "Takes in input signals and has an output signal\n",
    "<br>\n",
    "Inputs are usually normalized\n",
    "<br>\n",
    "Weights - connectors between the inputs and neurons and is how the network learns\n",
    "<br>\n",
    "Sum of the weights multiplied by the input values are taken, then is passed through an activation function\n",
    "<br>\n",
    "**Activation Functions**\n",
    "<br>\n",
    "Threshold: values < 0 are set to 0; values >=0 are set to 1\n",
    "<br>\n",
    "Sigmoid: 1 / 1 + e^(-x)\n",
    "* S shaped curve bounded by 0 and 1\n",
    "<br>\n",
    "Rectifier: values < 0 are set to 0; values >=0 follow y=x line\n",
    "<br>\n",
    "Hyperbolic tangent: 1-e^(-2x) / 1+e^(-2x)\n",
    "* Similar to sigmoid but the S shaped curve is bounded between -1 and 1\n",
    "<br>\n",
    "\n",
    "**Neural Networks**\n",
    "<br>\n",
    "Hidden layers allow the network to pick out specific attributes\n",
    "<br>\n",
    "Two different approaches (providing the AI with labeled data vs. having the AI learn on its own)\n",
    "<br>\n",
    "(supervised vs. unsupervised learning)\n",
    "<br>\n",
    "Cost function tells us the error in our prediction (which we try to minimize)\n",
    "<br>\n",
    "Weights are updated according to the cost function\n",
    "<br>\n",
    "Process: inputs fed into network -> weights are applied -> output is generated -> output is compared to actual value -> if cost/error/loss is significant: -> weights are adjusted -> inputs refed into network -> process restarts again\n",
    "<br>\n",
    "Gradient descent: finding the smallest value in a given function (used to minimize cost function)\n",
    "<br>\n",
    "Stochastic gradient descent: finds the global minimum value (whereas gradient descent may error in that it finds the local minimum)\n",
    "* Weights are readjusted after each input whereas in normal gradient descent (batch gradient descent) all inputs are fed into the network and THEN the weights are readjusted\n",
    "* Is actually faster than batch gradient descent\n",
    "\n",
    "Backpropagation: readjusts all the weights at once in order to minimize the cost function and better approach desired behavior\n",
    "<br>\n",
    "Epoch: one complete run through of all input values through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154aded",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "\n",
    "**Convolutional Neural Networks**\n",
    "<br>\n",
    "Feature maps are created by aligning kernels with the original image\n",
    "* Kernels are a small grid of numbers that constitute a feature\n",
    "* As the kernel passes over the image, any matches with a specific part of the kernel with the image is captured and stored in the feature map\n",
    "* Purpose is to decrease the size of the image while preserving special features and relationship among pixels\n",
    "* Feature map is then passed through ReLU (rectified linear unit) - an activation function\n",
    "<br>\n",
    "\n",
    "**Processing**\n",
    "<br>\n",
    "Pooling: The ability for the CNN to detect certain features despite distortions (feature is rotated, smaller than what the CNN trained on, etc...)\n",
    "<br>\n",
    "Max pooling: Feature map is shrunk and a small grid is passed over the original feature map\n",
    "* The biggest number in the grid is extracted and put into the corresponding slot\n",
    "* The ability for pooling to remove excess information is good because it prevents overfitting\n",
    "\n",
    "Mean pooling: Instead of taking the maximum value in the grid, take the average of all the values\n",
    "<br>\n",
    "Flattening: Take the values of the pooled feature map and align them into a column\n",
    "<br>\n",
    "The final steps involve connecting the column of values and passing them as inputs in an artifical neural network\n",
    "<br>\n",
    "When performing classification tasks, the output layer learns which neurons from the final fully connected layer (hidden layer in ANN terms) are important/relevant\n",
    "<br>\n",
    "* If certain neurons are producing a high value and they lead to a correct classification, then the network will listen to those specific neurons more when classifying\n",
    "* Conversely, if certain neurons are generating a high value but they lead to an incorrect classification, then those neurons will be ignored\n",
    "<br>\n",
    "\n",
    "**Performance/Error Analysis**\n",
    "<br>\n",
    "Softmax function: Takes values and squeezes them between 0 and 1 so that their collective sum equals 1\n",
    "<br>\n",
    "Important for CNN classification tasks (and other applications) in order to generate a probability that makes sense (Dog: 95%, Cat: 5% vs. Dog: 80%, Cat: 45%)\n",
    "<br>\n",
    "Cross-entropy function: loss function but for CNNs (cost function for ANNs)\n",
    "* Cross-entropy is also great for detecting improvements that may be subtle, difficult to notice, or regarded as only small improvements under MSE but is in reality a huge improvement (0.0000001 -> 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d425b",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "\n",
    "**Autoencoders**\n",
    "<br>\n",
    "Autoencoders aim to make the output layers generate values similar to the input layer\n",
    "* Encode data to make inputs take up less space\n",
    "* Important features are extracted while lesser important features are disregarded\n",
    "* Result are the remaining important features which do not take up as much space as the original input due to lesser important features being disregarded\n",
    "* Uses ANNs as usual\n",
    "\n",
    "Overcomplete hidden layers - more nodes in the hidden layers than the input layer\n",
    "* Slightly confused on the significance of this\n",
    "* Problem: network can cheat by simply passing information from the input layer directly to the next node in the hidden layer and then into the output layer\n",
    "* Various types of autoencoders exist to address this issue\n",
    "\n",
    "**Types of autoencoders**\n",
    "<br>\n",
    "Sparse autoencoders\n",
    "* Very popular type of autoencoder\n",
    "* At any time, the autoencoder can only use a portion of the total amount of hidden nodes (ex: nodes ABC out of the 8 total nodes for the first pass, nodes DEF out of the 8 total nodes for the second pass, nodes ACF out of the 8 total nodes for the third pass, etc)\n",
    "* Different hidden nodes are activated/deactivated for each epoch\n",
    "* The amount of hidden nodes activated at any given time will be lesser than the input layer\n",
    "\n",
    "Denoising autoencoders\n",
    "* Some input nodes are converted to 0\n",
    "\n",
    "Contractive autoencoders\n",
    "* A penalty is applied if the network simply copies the input into the output\n",
    "\n",
    "Stacked autoencoders\n",
    "* Adds an additional hidden layer\n",
    "\n",
    "Deep autoencoders\n",
    "* Restricted boltzmann machines stacked on top of each other (?)\n",
    "\n",
    "## Section 5\n",
    "\n",
    "**Variational Autoencoders**\n",
    "<br>\n",
    "Variational autoencoders are a type of encoder where \"dreams' are created to help the AI learn\n",
    "* These \"dreams\" are just variations in training that improves the learning process\n",
    "* Ex: If an AI is learning how to drive a car through a race track, then the \"dreams\" may feature altered versions of the race track which help the AI learn in various environments\n",
    "\n",
    "**Structure**\n",
    "<br>\n",
    "Instead of passing inputs into the latent vector (the compressed hidden layer), the inputs are first passed into a mean vector and a standard deviation vector\n",
    "* The values from those two vectors are then passed into the latent vector, which is then passed into the output layer\n",
    "* This process enables the values passed into the latent vector to be stochastic/varying\n",
    "\n",
    "Reparameterization\n",
    "<br>\n",
    "Since the latent vector will vary every time, this causes backpropagation to fail\n",
    "* To solve this, the \"randomness\" component is separated from the neural network\n",
    "* This way, backpropagation can continue to improve the network while randomness is being introduced as a separate component\n",
    "* Input -> Mean + Standard Deviation -> Latent -> Output\n",
    "* Input -> Mean + Standard Deviation + Random -> Latent -> Output\n",
    "* This way, part of the learning process the neural network has to go through is learning to adapt to this randomness introduced\n",
    "* This is where the \"dreams\" comes from (I believe); the network \"dreams\", or creates variations in its training environment to help better its learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

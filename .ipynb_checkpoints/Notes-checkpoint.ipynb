{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfbbf2c",
   "metadata": {},
   "source": [
    "## Section 1\n",
    "Hybrid models - combines different AI models together\n",
    "* CNN, MDN, GA, VAE, RNN, ES\n",
    "* Very powerful but computationally expensive to train\n",
    "\n",
    "**Useful resources:**\n",
    "<br>\n",
    "worldmodels.github.io\n",
    "<br>\n",
    "blog.otoro.net\n",
    "\n",
    "## Section 2\n",
    "\n",
    "**Neurons**\n",
    "<br>\n",
    "Recreation of a human neuron\n",
    "<br>\n",
    "Takes in input signals and has an output signal\n",
    "<br>\n",
    "Inputs are usually normalized\n",
    "<br>\n",
    "Weights - connectors between the inputs and neurons and is how the network learns\n",
    "<br>\n",
    "Sum of the weights multiplied by the input values are taken, then is passed through an activation function\n",
    "<br>\n",
    "**Activation Functions**\n",
    "<br>\n",
    "Threshold: values < 0 are set to 0; values >=0 are set to 1\n",
    "<br>\n",
    "Sigmoid: 1 / 1 + e^(-x)\n",
    "* S shaped curve bounded by 0 and 1\n",
    "<br>\n",
    "Rectifier: values < 0 are set to 0; values >=0 follow y=x line\n",
    "<br>\n",
    "Hyperbolic tangent: 1-e^(-2x) / 1+e^(-2x)\n",
    "* Similar to sigmoid but the S shaped curve is bounded between -1 and 1\n",
    "<br>\n",
    "\n",
    "**Neural Networks**\n",
    "<br>\n",
    "Hidden layers allow the network to pick out specific attributes\n",
    "<br>\n",
    "Two different approaches (providing the AI with labeled data vs. having the AI learn on its own)\n",
    "<br>\n",
    "(supervised vs. unsupervised learning)\n",
    "<br>\n",
    "Cost function tells us the error in our prediction (which we try to minimize)\n",
    "<br>\n",
    "Weights are updated according to the cost function\n",
    "<br>\n",
    "Process: inputs fed into network -> weights are applied -> output is generated -> output is compared to actual value -> if cost/error/loss is significant: -> weights are adjusted -> inputs refed into network -> process restarts again\n",
    "<br>\n",
    "Gradient descent: finding the smallest value in a given function (used to minimize cost function)\n",
    "<br>\n",
    "Stochastic gradient descent: finds the global minimum value (whereas gradient descent may error in that it finds the local minimum)\n",
    "* Weights are readjusted after each input whereas in normal gradient descent (batch gradient descent) all inputs are fed into the network and THEN the weights are readjusted\n",
    "* Is actually faster than batch gradient descent\n",
    "\n",
    "Backpropagation: readjusts all the weights at once in order to minimize the cost function and better approach desired behavior\n",
    "<br>\n",
    "Epoch: one complete run through of all input values through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154aded",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "\n",
    "**Convolutional Neural Networks**\n",
    "<br>\n",
    "Feature maps are created by aligning kernels with the original image\n",
    "* Kernels are a small grid of numbers that constitute a feature\n",
    "* As the kernel passes over the image, any matches with a specific part of the kernel with the image is captured and stored in the feature map\n",
    "* Purpose is to decrease the size of the image while preserving special features and relationship among pixels\n",
    "* Feature map is then passed through ReLU (rectified linear unit) - an activation function\n",
    "<br>\n",
    "\n",
    "**Processing**\n",
    "<br>\n",
    "Pooling: The ability for the CNN to detect certain features despite distortions (feature is rotated, smaller than what the CNN trained on, etc...)\n",
    "<br>\n",
    "Max pooling: Feature map is shrunk and a small grid is passed over the original feature map\n",
    "* The biggest number in the grid is extracted and put into the corresponding slot\n",
    "* The ability for pooling to remove excess information is good because it prevents overfitting\n",
    "\n",
    "Mean pooling: Instead of taking the maximum value in the grid, take the average of all the values\n",
    "<br>\n",
    "Flattening: Take the values of the pooled feature map and align them into a column\n",
    "<br>\n",
    "The final steps involve connecting the column of values and passing them as inputs in an artifical neural network\n",
    "<br>\n",
    "When performing classification tasks, the output layer learns which neurons from the final fully connected layer (hidden layer in ANN terms) are important/relevant\n",
    "<br>\n",
    "* If certain neurons are producing a high value and they lead to a correct classification, then the network will listen to those specific neurons more when classifying\n",
    "* Conversely, if certain neurons are generating a high value but they lead to an incorrect classification, then those neurons will be ignored\n",
    "<br>\n",
    "\n",
    "**Performance/Error Analysis**\n",
    "<br>\n",
    "Softmax function: Takes values and squeezes them between 0 and 1 so that their collective sum equals 1\n",
    "<br>\n",
    "Important for CNN classification tasks (and other applications) in order to generate a probability that makes sense (Dog: 95%, Cat: 5% vs. Dog: 80%, Cat: 45%)\n",
    "<br>\n",
    "Cross-entropy function: loss function but for CNNs (cost function for ANNs)\n",
    "* Cross-entropy is also great for detecting improvements that may be subtle, difficult to notice, or regarded as only small improvements under MSE but is in reality a huge improvement (0.0000001 -> 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d425b",
   "metadata": {},
   "source": [
    "## Section 4\n",
    "\n",
    "**Autoencoders**\n",
    "<br>\n",
    "Autoencoders aim to make the output layers generate values similar to the input layer\n",
    "* Encode data to make inputs take up less space\n",
    "* Important features are extracted while lesser important features are disregarded\n",
    "* Result are the remaining important features which do not take up as much space as the original input due to lesser important features being disregarded\n",
    "* Uses ANNs as usual\n",
    "\n",
    "Overcomplete hidden layers - more nodes in the hidden layers than the input layer\n",
    "* Slightly confused on the significance of this\n",
    "* Problem: network can cheat by simply passing information from the input layer directly to the next node in the hidden layer and then into the output layer\n",
    "* Various types of autoencoders exist to address this issue\n",
    "\n",
    "**Types of autoencoders**\n",
    "<br>\n",
    "Sparse autoencoders\n",
    "* Very popular type of autoencoder\n",
    "* At any time, the autoencoder can only use a portion of the total amount of hidden nodes (ex: nodes ABC out of the 8 total nodes for the first pass, nodes DEF out of the 8 total nodes for the second pass, nodes ACF out of the 8 total nodes for the third pass, etc)\n",
    "* Different hidden nodes are activated/deactivated for each epoch\n",
    "* The amount of hidden nodes activated at any given time will be lesser than the input layer\n",
    "\n",
    "Denoising autoencoders\n",
    "* Some input nodes are converted to 0\n",
    "\n",
    "Contractive autoencoders\n",
    "* A penalty is applied if the network simply copies the input into the output\n",
    "\n",
    "Stacked autoencoders\n",
    "* Adds an additional hidden layer\n",
    "\n",
    "Deep autoencoders\n",
    "* Restricted boltzmann machines stacked on top of each other (?)\n",
    "\n",
    "## Section 5\n",
    "\n",
    "**Variational Autoencoders**\n",
    "<br>\n",
    "Variational autoencoders are a type of encoder where \"dreams' are created to help the AI learn\n",
    "* These \"dreams\" are just variations in training that improves the learning process\n",
    "* Ex: If an AI is learning how to drive a car through a race track, then the \"dreams\" may feature altered versions of the race track which help the AI learn in various environments\n",
    "\n",
    "**Structure**\n",
    "<br>\n",
    "Instead of passing inputs into the latent vector (the compressed hidden layer), the inputs are first passed into a mean vector and a standard deviation vector\n",
    "* The values from those two vectors are then passed into the latent vector, which is then passed into the output layer\n",
    "* This process enables the values passed into the latent vector to be stochastic/varying\n",
    "\n",
    "Reparameterization\n",
    "<br>\n",
    "Since the latent vector will vary every time, this causes backpropagation to fail\n",
    "* To solve this, the \"randomness\" component is separated from the neural network\n",
    "* This way, backpropagation can continue to improve the network while randomness is being introduced as a separate component\n",
    "* Input -> Mean + Standard Deviation -> Latent -> Output\n",
    "* Input -> Mean + Standard Deviation + Random -> Latent -> Output\n",
    "* This way, part of the learning process the neural network has to go through is learning to adapt to this randomness introduced\n",
    "* This is where the \"dreams\" comes from (I believe); the network \"dreams\", or creates variations in its training environment to help better its learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de46bb",
   "metadata": {},
   "source": [
    "## Section 6\n",
    "\n",
    "**Implementing the CNN-VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN-VAE model\n",
    " \n",
    "# Importing the libraries\n",
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "# Building the CNN-VAE model within a class\n",
    " \n",
    "class ConvVAE(object):\n",
    "    \n",
    "    # Initializing all the parameters and variables of the ConvVAE class\n",
    "    def __init__(self, z_size=32, batch_size=1, learning_rate=0.0001, kl_tolerance=0.5, is_training=False, reuse=False, gpu_mode=False):\n",
    "        self.z_size = z_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "        self.is_training = is_training\n",
    "        self.reuse = reuse\n",
    "        with tf.variable_scope('conv_vae', reuse=self.reuse):\n",
    "            if not gpu_mode:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    tf.logging.info('Model using cpu.')\n",
    "                    self._build_graph()\n",
    "            else:\n",
    "                tf.logging.info('Model using gpu.')\n",
    "                self._build_graph()\n",
    "        self._init_session()\n",
    "    \n",
    "    # Making a method that creates the VAE model architecture itself\n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "            # Building the Encoder part of the VAE\n",
    "            h = tf.layers.conv2d(self.x, 32, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv1\")\n",
    "            h = tf.layers.conv2d(h, 64, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv2\")\n",
    "            h = tf.layers.conv2d(h, 128, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv3\")\n",
    "            h = tf.layers.conv2d(h, 256, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv4\")\n",
    "            h = tf.reshape(h, [-1, 2*2*256])\n",
    "            # Building the \"V\" part of the VAE\n",
    "            self.mu = tf.layers.dense(h, self.z_size, name=\"enc_fc_mu\")\n",
    "            self.logvar = tf.layers.dense(h, self.z_size, name=\"enc_fc_log_var\")\n",
    "            self.sigma = tf.exp(self.logvar / 2.0)\n",
    "            self.epsilon = tf.random_normal([self.batch_size, self.z_size])\n",
    "            self.z = self.mu + self.sigma * self.epsilon\n",
    "            # Building the Decoder part of the VAE\n",
    "            h = tf.layers.dense(self.z, 1024, name=\"dec_fc\")\n",
    "            # Convert into 1 dimensional vector of 1024 elements\n",
    "            h = tf.reshape(h, [-1, 1, 1, 1024])\n",
    "            # Reconstruct back into inputs\n",
    "            h = tf.layers.conv2d_transpose(h, 128, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv1\")\n",
    "            h = tf.layers.conv2d_transpose(h, 64, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv2\")\n",
    "            h = tf.layers.conv2d_transpose(h, 32, 6, strides=2, activation=tf.nn.relu, name=\"dec_deconv3\")\n",
    "            self.y = tf.layers.conv2d_transpose(h, 3, 6, strides=2, activation=tf.nn.sigmoid, name=\"dec_deconv4\")\n",
    "            # Implementing the training operations\n",
    "            if self.is_training:\n",
    "                # Stepping closer to global min (gradient descent)\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                # Mean squared error (self.y is predicted, self.x is actual)\n",
    "                self.r_loss = tf.reduce_sum(tf.square(self.x - self.y), reduction_indices = [1,2,3])\n",
    "                # Mean part of mean squared error\n",
    "                self.r_loss = tf.reduce_mean(self.r_loss)\n",
    "                # 2nd loss function: KL error\n",
    "                self.kl_loss = - 0.5 * tf.reduce_sum((1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)), reduction_indices = 1)\n",
    "                self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)\n",
    "                self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "                # KL error and mean squared errors are used together\n",
    "                self.loss = self.r_loss + self.kl_loss\n",
    "                # Learning rate\n",
    "                self.lr = tf.Variable(self.learning_rate, trainable=False)\n",
    "                # Use Adam as optimizer for gradient descent\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "                # Calculate gradients\n",
    "                grads = self.optimizer.compute_gradients(self.loss)\n",
    "                self.train_op = self.optimizer.apply_gradients(grads, global_step=self.global_step, name='train_step')\n",
    "            self.init = tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfbbf2c",
   "metadata": {},
   "source": [
    "## Section 1\n",
    "Hybrid models - combines different AI models together\n",
    "* CNN, MDN, GA, VAE, RNN, ES\n",
    "* Very powerful but computationally expensive to train\n",
    "\n",
    "**Useful resources:**\n",
    "<br>\n",
    "worldmodels.github.io\n",
    "<br>\n",
    "blog.otoro.net\n",
    "\n",
    "## Section 2\n",
    "\n",
    "**Neurons**\n",
    "<br>\n",
    "Recreation of a human neuron\n",
    "<br>\n",
    "Takes in input signals and has an output signal\n",
    "<br>\n",
    "Inputs are usually normalized\n",
    "<br>\n",
    "Weights - connectors between the inputs and neurons and is how the network learns\n",
    "<br>\n",
    "Sum of the weights multiplied by the input values are taken, then is passed through an activation function\n",
    "<br>\n",
    "**Activation Functions**\n",
    "<br>\n",
    "Threshold: values < 0 are set to 0; values >=0 are set to 1\n",
    "<br>\n",
    "Sigmoid: 1 / 1 + e^(-x)\n",
    "* S shaped curve bounded by 0 and 1\n",
    "<br>\n",
    "Rectifier: values < 0 are set to 0; values >=0 follow y=x line\n",
    "<br>\n",
    "Hyperbolic tangent: 1-e^(-2x) / 1+e^(-2x)\n",
    "* Similar to sigmoid but the S shaped curve is bounded between -1 and 1\n",
    "<br>\n",
    "\n",
    "**Neural Networks**\n",
    "<br>\n",
    "Hidden layers allow the network to pick out specific attributes\n",
    "<br>\n",
    "Two different approaches (providing the AI with labeled data vs. having the AI learn on its own)\n",
    "<br>\n",
    "(supervised vs. unsupervised learning)\n",
    "<br>\n",
    "Cost function tells us the error in our prediction (which we try to minimize)\n",
    "<br>\n",
    "Weights are updated according to the cost function\n",
    "<br>\n",
    "Process: inputs fed into network -> weights are applied -> output is generated -> output is compared to actual value -> if cost/error/loss is significant: -> weights are adjusted -> inputs refed into network -> process restarts again\n",
    "<br>\n",
    "Gradient descent: finding the smallest value in a given function (used to minimize cost function)\n",
    "<br>\n",
    "Stochastic gradient descent: finds the global minimum value (whereas gradient descent may error in that it finds the local minimum)\n",
    "* Weights are readjusted after each input whereas in normal gradient descent (batch gradient descent) all inputs are fed into the network and THEN the weights are readjusted\n",
    "* Is actually faster than batch gradient descent\n",
    "\n",
    "Backpropagation: readjusts all the weights at once in order to minimize the cost function and better approach desired behavior\n",
    "<br>\n",
    "Epoch: one complete run through of all input values through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154aded",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "\n",
    "**Convolutional Neural Networks**\n",
    "<br>\n",
    "Feature maps are created by aligning kernels with the original image\n",
    "* Kernels are a small grid of numbers that constitute a feature\n",
    "* As the kernel passes over the image, any matches with a specific part of the kernel with the image is captured and stored in the feature map\n",
    "* Purpose is to decrease the size of the image while preserving special features and relationship among pixels\n",
    "* Feature map is then passed through ReLU (rectified linear unit) - an activation function\n",
    "<br>\n",
    "\n",
    "**Processing**\n",
    "<br>\n",
    "Pooling: The ability for the CNN to detect certain features despite distortions (feature is rotated, smaller than what the CNN trained on, etc...)\n",
    "<br>\n",
    "Max pooling: Feature map is shrunk and a small grid is passed over the original feature map\n",
    "* The biggest number in the grid is extracted and put into the corresponding slot\n",
    "* The ability for pooling to remove excess information is good because it prevents overfitting\n",
    "\n",
    "Mean pooling: Instead of taking the maximum value in the grid, take the average of all the values\n",
    "<br>\n",
    "Flattening: Take the values of the pooled feature map and align them into a column\n",
    "<br>\n",
    "The final steps involve connecting the column of values and passing them as inputs in an artifical neural network\n",
    "<br>\n",
    "When performing classification tasks, the output layer learns which neurons from the final fully connected layer (hidden layer in ANN terms) are important/relevant\n",
    "<br>\n",
    "* If certain neurons are producing a high value and they lead to a correct classification, then the network will listen to those specific neurons more when classifying\n",
    "* Conversely, if certain neurons are generating a high value but they lead to an incorrect classification, then those neurons will be ignored\n",
    "<br>\n",
    "\n",
    "**Performance/Error Analysis**\n",
    "<br>\n",
    "Softmax function: Takes values and squeezes them between 0 and 1 so that their collective sum equals 1\n",
    "<br>\n",
    "Important for CNN classification tasks (and other applications) in order to generate a probability that makes sense (Dog: 95%, Cat: 5% vs. Dog: 80%, Cat: 45%)\n",
    "<br>\n",
    "Cross-entropy function: loss function but for CNNs (cost function for ANNs)\n",
    "* Cross-entropy is also great for detecting improvements that may be subtle, difficult to notice, or regarded as only small improvements under MSE but is in reality a huge improvement (0.0000001 -> 0.0001)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
